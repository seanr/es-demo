<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE xml>
<!--
Copyright 2010-2017 Norconex Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!--
NOTE: No usage for independant crawler configuration
-->

<!-- Variables: Optionally define variables in this configuration file
using the "set" directive, or by using a file of the same name
but with the extension ".variables" or ".properties".
-->
#set($core      = "com.norconex.collector.core")
#set($http      = "com.norconex.collector.http")
#set($committer = "com.norconex.committer")
#set($importer  = "com.norconex.importer")

#set($httpClientFactory = "${http}.client.impl.GenericHttpClientFactory")
#set($filterExtension   = "${core}.filter.impl.ExtensionReferenceFilter")
#set($filterRegexRef    = "${core}.filter.impl.RegexReferenceFilter")
#set($filterRegexMeta   = "${core}.filter.impl.RegexMetadataFilter")
#set($robotsTxt         = "${http}.robot.impl.StandardRobotsTxtProvider")
#set($robotsMeta        = "${http}.robot.impl.StandardRobotsMetaProvider")
#set($redirectProvider  = "${http}.redirect.impl.GenericRedirectURLProvider")
#set($recrawlResolver   = "${http}.recrawl.impl.GenericRecrawlableResolver")
#set($metaFetcher       = "${http}.fetch.impl.GenericMetadataFetcher")
#set($docFetcher        = "${http}.fetch.impl.GenericDocumentFetcher")
#set($linkExtractor     = "${http}.url.impl.GenericLinkExtractor")
#set($canonLinkDetector = "${http}.url.impl.GenericCanonicalLinkDetector")
#set($urlNormalizer     = "${http}.url.impl.GenericURLNormalizer")
#set($delayResolver     = "${http}.delay.impl.GenericDelayResolver")
#set($sitemapFactory    = "${http}.sitemap.impl.StandardSitemapResolverFactory")
<!-- #set($metaChecksummer   = "${http}.checksum.impl.HttpMetadataChecksummer") -->
#set($lastChecksummer   = "${http}.checksum.impl.LastModifiedMetadataChecksummer")
#set($MD5Checksummer    = "${core}.checksum.impl.MD5DocumentChecksummer")
<!-- # set($dataStoreFactory  = "${core}.data.store.impl.mvstore.MVStoreCrawlDataStoreFactory") -->
#set($mongoStoreFactory = "${http}.data.store.impl.mongo.MongoCrawlDataStoreFactory")
#set($spoiledStrategy   = "${core}.spoil.impl.GenericSpoiledReferenceStrategizer")
#set($fsCommitter       = "${committer}.core.impl.FileSystemCommitter")

<!-- All crawler configuration options can be specified as default
     (including start URLs).  Settings defined here will be inherited by
     all individual crawlers defined further down, unless overwritten.
     If you replace a top level crawler tag from the crawler defaults,
     all the default tag configuration settings will be replaced, no
     attempt will be made to merge or append.
     -->
<crawlerDefaults>
  <!-- Identify yourself to sites you crawl.  It sets the "User-Agent" HTTP
       request header value.  This is how browsers identify themselves for
       instance.  Sometimes required to be certain values for robots.txt
       files.
    -->
  <userAgent>Mozilla/5.0 (compatible; webolutionarycrawler/0.1; +http://webolutionary.com/webolutionarycrawler)</userAgent>

  <!-- Optional URL normalization feature. The class must implement
       com.norconex.collector.http.url.IURLNormalizer,
       like the following class does.
    -->
  <urlNormalizer class="$urlNormalizer">
    <normalizations>
      lowerCaseSchemeHost, upperCaseEscapeSequence, decodeUnreservedCharacters, removeDefaultPort,
      addDomainTrailingSlash, removeDuplicateSlashes
    </normalizations>
    <replacements>
      <replace>
         <match>&amp;view=print</match>
         <replacement>&amp;view=html</replacement>
      </replace>
    </replacements>
  </urlNormalizer>

  <!-- Optional delay resolver defining how polite or aggressive you want
       your crawling to be.  The class must implement
       com.norconex.collector.http.delay.IDelayResolver.
       The following class is the default implementation
       (but the schedule sample is not):
    -->
  <delay class="$delayResolver" default="2000" ignoreRobotsCrawlDelay="true">
    <schedule dayOfWeek="from Saturday to Sunday">2 second</schedule>
  </delay>

  <!-- How many threads you want a crawler to use.  Regardless of how many
       thread you have running, the frequency of each URL being invoked
       will remain dictated by the &lt;delay/&gt option above.  Using more
       than one thread is a good idea to ensure the delay is respected
       in case you run into test downloads taking more time than the
       delay specified. Default is 2 threads.
    -->
  <numThreads>4</numThreads>

  <!-- How many level deep can the crawler go. I.e, within how many clicks
       away from the main page (start URL) each page can be to be considered.
       Beyond the depth specified, pages are rejected.
       The starting URLs all have a zero-depth.  Default is -1 (unlimited)
       -->
  <maxDepth>-1</maxDepth>

  <!-- Stop crawling after how many successfully processed documents.
       A successful document is one that is either new or modified, that was
       not rejected, not deleted, or did not generate any error.  As an
       example, this is a document that will end up in your search engine.
       Default is -1 (unlimited)
        -->
  <maxDocuments>-1</maxDocuments>

  <!-- Crawler "work" directory.  This is where files downloaded or created as
       part of crawling activities (besides logs and progress) get stored.
       It should be unique to each crawlers.
       -->
  <workDir>${workdir}</workDir>

  <referenceFilters>
    <filter class="$filterExtension" onMatch="exclude">jpg,jpeg,gif,png,bmp,tif,tiff,ico,css,js,exe,zip,tar,fla</filter>
    <filter class="$filterRegexRef" onMatch="include" caseSensitive="false"><![CDATA[${crawler-url}.*]]></filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="false"><![CDATA[${crawler-url}.*\.\..*]]></filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="false"><![CDATA[.*\?(?!page=).*]]></filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true"><![CDATA[.*\#.*]]></filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true">.*/node/.*</filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true">.*/taxonomy/.*</filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true">.*/user/.*</filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true">.*/admin.*</filter>
    <filter class="$filterRegexRef" onMatch="exclude" caseSensitive="true">.*/panels/.*</filter>
  </referenceFilters>

  <!-- Keep downloaded files. Default is false. -->
  <keepDownloads>false</keepDownloads>

  <!-- Keep extracted links that are out-of-scope according to start URL
       stayOn... flags (if any are true).  Out-of-scope links are
       stored in a metadata field.
       -->
  <keepOutOfScopeLinks>false</keepOutOfScopeLinks>

  <!-- What to do with orphan documents.  Orphans are valid
       documents, which on subsequent crawls can no longer be reached when
       running the crawler (e.g. there are no links pointing to that page
       anymore).  Available options are:
       IGNORE, DELETE, and PROCESS (default).
       -->
  <orphansStrategy>DELETE</orphansStrategy>

  <!-- One or more fully qualified names of Java exceptions
       that should force a crawler to stop when triggered during the
       processing of a document.
       Default is empty (will try to continue upon exceptions).
       -->
  <stopOnExceptions>
    <exception>com.norconex.committer.core.CommitterException</exception>
  </stopOnExceptions>

  <!-- Provides the target URL to use when a redirect is encountered.
       Classes must implement *.redirect.IRedirectURLProvider.
       Default implementation is the following.
       -->
  <redirectURLProvider class="$redirectProvider" fallbackCharset="" />

  <!-- Indicates if a target URL is ready for recrawl or not.
       Default implementation is the following.
       -->
  <recrawlableResolver class="$recrawlResolver" />

  <!-- Fetch a URL HTTP Headers.  Classes must implement
       com.norconex.collector.http.fetch.IHttpMetadataFetcher.
       The following is a simple implementation.
       HTTP headers are normally fetched by the "documentFetcher". Use
       this only when necessary (see documentation).
       -->
  <metadataFetcher class="$metaFetcher">
    <validStatusCodes>200</validStatusCodes>
  </metadataFetcher>

  <!-- Optionally filter AFTER download of HTTP headers.  Classes must
       implement com.norconex.collector.core.filter.IMetadataFilter.
       -->
  <metadataFilters>
    <!-- Do not index content-type of CSS or JavaScript -->
    <filter class="$filterRegexMeta" onMatch="exclude" caseSensitive="false" field="Content-Type">.*css.*|.*javascript.*|.*map.*</filter>
  </metadataFilters>

  <!-- Detect canonical links. Classes must implement
       com.norconex.collector.http.url.ICanonicalLinkDetector.
       Default implementation is the following.
       -->
  <canonicalLinkDetector ignore="true" />
  <!--
  <canonicalLinkDetector ignore="false" class="${canonLinkDetector}">
    <contentTypes>text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp</contentTypes>
  </canonicalLinkDetector>
  -->

  <!-- Fetches document.  Class must implement
       com.norconex.collector.http.fetch.IHttpDocumentFetcher.
       Default implementation is the following.
       -->
  <documentFetcher class="$docFetcher" detectContentType="true" detectCharset="true">
    <validStatusCodes>200</validStatusCodes>
    <notFoundStatusCodes>404</notFoundStatusCodes>
  </documentFetcher>

  <!-- Establish whether to follow a page URLs or to index a given page
       based on in-page meta tag robot information. Classes must implement
       com.norconex.collector.http.robot.IRobotsMetaProvider.
       Default implementation is the following.
       -->
  <robotsMeta ignore="false" class="$robotsMeta" />
  <robotsTxt ignore="false" class="$robotsTxt" />

  <!-- Loads sitemap.xml URLs and adds adds them to URLs to process -->
  <sitemapResolverFactory class="$sitemapFactory" ignore="false" lenient="true">
    <tempDir>${workdir}/sitemaps</tempDir>
    <path>${crawler-url}sitemap.xml</path>
  </sitemapResolverFactory>

  <!-- Extract links from a document.  Classes must implement
       com.norconex.collector.http.url.ILinkExtractor.
       Default implementation is the following.
       -->
  <linkExtractors>
    <extractor class="$linkExtractor" maxURLLength="2048" ignoreNofollow="false" commentsEnabled="false">
      <contentTypes>
        text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp
      </contentTypes>
      <tags>
        <tag name="a" attribute="href" />
        <tag name="frame" attribute="src" />
        <tag name="iframe" attribute="src" />
        <tag name="img" attribute="src" />
        <tag name="meta" attribute="http-equiv" />
      </tags>
    </extractor>
  </linkExtractors>

  <httpClientFactory class="${http}.client.impl.GenericHttpClientFactory">
    <cookiesDisabled>false</cookiesDisabled>
    <trustAllSSLCertificates>true</trustAllSSLCertificates>
  </httpClientFactory>

  <!-- Generates a checksum value from document headers to find out if
       a document has changed. Class must implement
       com.norconex.collector.core.checksum.IMetadataChecksummer.
       Default implementation is the following.
       -->
  <metadataChecksummer class="$lastChecksummer" disabled="false" keep="false" targetField="collector.checksum-metadata" />
  <documentChecksummer class="$MD5Checksummer">
    <!-- <sourceFields>Last-Modified</sourceFields> -->
    <sourceFields>content, title, description</sourceFields>
  </documentChecksummer>

  <!-- Decide what to do with references that have turned bad.
       Class must implement
       com.norconex.collector.core.spoil.ISpoiledReferenceStrategizer.
       Default implementation is the following.
       -->
  <spoiledReferenceStrategizer class="$spoiledStrategy" fallbackStrategy="DELETE">
    <mapping state="NOT_FOUND"  strategy="DELETE" />
    <mapping state="BAD_STATUS" strategy="GRACE_ONCE" />
    <mapping state="ERROR"      strategy="GRACE_ONCE" />
  </spoiledReferenceStrategizer>

</crawlerDefaults>
